{"cells":[{"outputs":[{"output_type":"stream","text":"ptb_train1020\r\n","name":"stdout"}],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0C451BB6B30647A991D1B159DD33E46C","scrolled":false}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D6C7A91A5DBC43B6A0B539AB84DD0B15"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"138DB395F7EA4A30BBD3D019AC5D6D93"}},{"metadata":{"id":"73A4AD711DCF4527A1634197AF092F6B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as Data\nimport math\nimport numpy as np\nimport random\nimport collections\nimport time","execution_count":1},{"metadata":{"id":"E0A488074E8A4AAA8355814DA900DB77","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"9858"},"transient":{},"execution_count":2}],"source":"with open('/home/kesci/input/ptb_train1020/ptb.train.txt', 'r') as f:\n    corpus = f.readlines()\n    # print(corpus[:3])\n    corpus = [line.strip().split(' ') for line in corpus]\n    # print(corpus[:3])\n\ndef get_vocab(corpus, min_freq):\n    tokens = [token for sentence in corpus for token in sentence]\n    counter = collections.Counter(tokens)\n    token_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n    idx2token = [token for token, freq in token_freq if freq >= min_freq]\n    token2idx = {}\n    for index, token in enumerate(idx2token):\n        token2idx[token] = index\n    vocab_size = len(idx2token)\n    return counter, idx2token, token2idx, vocab_size\n\ncounter, idx2token, token2idx, vocab_size = get_vocab(corpus, 5)\nvocab_size\n\n    ","execution_count":2},{"metadata":{"id":"6BE3C036A46F44DE843ED22544AC1B0F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"dataset = [[token2idx[token] for token in sentence if token in idx2token] for sentence in corpus]\nnum_tokens = sum([len(sentence) for sentence in dataset])","execution_count":3},{"metadata":{"id":"4E9932C3BC0A4A40889608B8E50ABF12","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"887100"},"transient":{},"execution_count":4}],"source":"num_tokens","execution_count":4},{"metadata":{"id":"8F96A72B482D42108932B07904DD7465","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[[],\n [8568, 1, 2, 71, 392, 32, 2115, 0, 145, 18, 5, 8569, 274, 406, 2],\n [22, 1, 12, 140, 3, 1, 5277, 0, 3054, 1580, 95]]"},"transient":{},"execution_count":5}],"source":"dataset[:3]","execution_count":5},{"metadata":{"id":"7E3D674ECF1640A5808F7DEFAE707961","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[[], [8568, 71, 392, 2115, 8569, 406], [140, 5277, 3054, 1580]]"},"transient":{},"execution_count":6}],"source":"## 降采样\ndef discard(idx):\n    return np.random.uniform(0, 1) < 1 - math.sqrt(1e-4 / counter[idx2token[idx]]* num_tokens)\n\nsubsampling_dataset = [[token for token in sentence if not discard(token)] for sentence in dataset]\nsubsampling_dataset[:3]","execution_count":6},{"metadata":{"id":"3A54606B6D2242D68B809F4A9BA27A2D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"before subsampling, the number of the is 50770\nafter subsampling, the number of the is 2150\nbefore subsampling, the number of <unk> is 45020\nafter subsampling, the number of <unk> is 2030\n","name":"stdout"}],"source":"def compare_num_tokens(token):\n    before = sum([sentence.count(token2idx[token]) for sentence in dataset])\n    after = sum([sentence.count(token2idx[token]) for sentence in subsampling_dataset])\n    print('before subsampling, the number of %s is %s'%(token, before))\n    print('after subsampling, the number of %s is %s'%(token, after))\ncompare_num_tokens('the')\ncompare_num_tokens('<unk>')\n    ","execution_count":7},{"metadata":{"id":"724180D8193545928AB23EF6829EF10D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"True\n","name":"stdout"}],"source":"def get_context(sentence, center, max_window_size):\n    window_size = np.random.randint(1, max_window_size + 1)\n    start_idx = max(0, center - max_window_size)\n    end_idx = min(len(sentence), center + max_window_size)\n    return sentence[start_idx: center] + sentence[center + 1: end_idx + 1]\n\ndef get_contexts(dataset, max_window_size):\n    contexts, centers = [], []\n    for sentence in dataset:\n        if len(sentence) < 2:\n            continue\n        centers += sentence\n        for center_idx in range(len(sentence)):\n            context = get_context(sentence, center_idx, max_window_size)\n            contexts.append(context)\n    return centers, contexts\n\nall_centers, all_contexts = get_contexts(subsampling_dataset, 5)\nprint(len(all_centers)==len(all_contexts))\n\n# 测试get_contexts函数\n# tiny = [list(range(4,10)), list(range(3,8))]\n# print(tiny)\n# a, b = get_contexts(tiny, 2)\n# for center, context in zip(a,b):\n#     print('center %s -- context %s'%(center, context))\n\n","execution_count":8},{"metadata":{"id":"CE3CD6440DF24DB9891662BAAD8180B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##trick  考虑到挑选出negative词之后还要舍去在contexts中的词，因此可以先选出很多negative，\n##       然后按顺序放进negative列表中。\n\ndef get_negatives(contexts, sampling_weights, K):\n    negatives, neg_candidates = [], []\n    i = 0\n    for context in contexts:\n        negative = []\n        while len(negative) < len(context) * K:\n            if i == len(neg_candidates):\n                neg_candidates = random.choices(list(range(len(sampling_weights))), sampling_weights, k=int(1e5))\n                i = 0\n            if neg_candidates[i] not in context:\n                negative.append(neg_candidates[i])\n            i += 1\n        negatives.append(negative)\n    return negatives\n\nsampling_weights = [counter[token]**0.75 for token in idx2token]\nall_negatives = get_negatives(all_contexts, sampling_weights, 5)\n\n        \n        \n    \n    ","execution_count":9},{"metadata":{"id":"2DBAD3E5EAD142988E88A7D6DC790F90","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[8568, 71, 392, 2115, 8569, 406]\n8568\n[71, 392, 2115, 8569, 406] 5\n[90, 8730, 0, 2904, 8, 4260, 358, 6343, 226, 18, 513, 2125, 1264, 1614, 29, 108, 4674, 2, 1153, 2832, 1425, 1725, 8468, 450, 5172] 25\nTrue\n","name":"stdout"}],"source":"print(subsampling_dataset[1])\nprint(all_centers[0])\nprint(all_contexts[0], len(all_contexts[0]))\nprint(all_negatives[0], len(all_negatives[0]))\nprint(len(all_contexts) == len(all_negatives))","execution_count":10},{"metadata":{"id":"8A8F6E1168F5473D896BFD23BB81F152","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def get_contexts_and_negatives(contexts, negatives):\n    max_len = max([len(context)+len(negative) for context, negative in zip(contexts, negatives)])\n    contexts_negatives, labels, masks = [], [], []\n    for context, negative in zip(contexts, negatives):\n        current_len = len(context) + len(negative)\n        context_negative = context + negative + [0] * (max_len - current_len)\n        label = [1] * len(context) + [0] * (max_len - len(context))\n        mask = [1] * current_len + [0] * (max_len - current_len)\n        contexts_negatives.append(context_negative)\n        labels.append(label)\n        masks.append(mask)\n    return torch.tensor(contexts_negatives), torch.tensor(labels), torch.tensor(masks)\n\ncontexts_negatives, labels, masks = get_contexts_and_negatives(all_contexts, all_negatives)\n\n# print(all_centers[1])\n# print(all_contexts[1])\n# print(all_negatives[1])\n# print(contexts_negatives[1])\n# print(labels[1])\n# print(masks[1])\n    ","execution_count":11},{"metadata":{"id":"2C8F9A92E37E4E448E7B4ACDFA4ECEE2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([512]) torch.Size([512, 60]) torch.Size([512, 60]) torch.Size([512, 60])\n","name":"stdout"}],"source":"data = Data.TensorDataset(torch.tensor(all_centers), contexts_negatives, labels, masks)\ndata_iter = Data.DataLoader(data, batch_size=512, shuffle=True)\n\nfor batch in data_iter:\n    print(batch[0].size(), batch[1].size(), batch[2].size(), batch[3].size())\n    break\n    ","execution_count":12},{"metadata":{"id":"C9B4CBC8DE5649FBACF6F3A09DC84474","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class SkipGram(nn.Module):\n    def __init__(self, vocab_size, embed_size):\n        super(SkipGram, self).__init__()\n        self.input_embedding = nn.Embedding(vocab_size, embed_size)\n        self.output_embedding = nn.Embedding(vocab_size, embed_size)\n    def forward(self, center, target):\n        center_embedding = self.input_embedding(center) #[batch_size, 1, embed_size]\n        target_embedding = self.output_embedding(target) #[batch_size, K, embed_size]\n        weight = torch.bmm(center_embedding, target_embedding.transpose(1, 2)) #[batch_size, 1, K]\n        return weight","execution_count":13},{"metadata":{"id":"4A25CD45F8CC4C8089DDFE1F26D693D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train_skip_gram(num_epochs, net, optimizer, loss_fn):\n    for epoch in range(num_epochs):\n        train_loss = 0\n        n = 0\n        start = time.time()\n        for center, context_negative, label, mask in data_iter:\n            optimizer.zero_grad()\n            weight = net(center.view(-1, 1), context_negative).squeeze(1)\n            masked_weight = torch.mul(weight, mask)\n            loss = (loss_fn(masked_weight.float(), label.float()).sum(dim=1)/mask.float().sum(dim=1)).mean()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            n += 1\n        print('Epoch %s , Train Loss %.5f, Used Time %s'%(epoch, train_loss/n, time.time()-start))\n            ","execution_count":14},{"metadata":{"id":"F0196F0DD8C540B281D0DEE2B56616BD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Epoch 0 , Train Loss 2.21965, Used Time 68.63986110687256\nEpoch 1 , Train Loss 1.05805, Used Time 69.09787106513977\nEpoch 2 , Train Loss 0.93120, Used Time 68.80610537528992\nEpoch 3 , Train Loss 0.89106, Used Time 68.78967523574829\nEpoch 4 , Train Loss 0.87243, Used Time 69.08850312232971\nEpoch 5 , Train Loss 0.86118, Used Time 69.01385641098022\nEpoch 6 , Train Loss 0.85283, Used Time 68.99070835113525\nEpoch 7 , Train Loss 0.84597, Used Time 68.91781497001648\nEpoch 8 , Train Loss 0.84019, Used Time 69.29813933372498\nEpoch 9 , Train Loss 0.83522, Used Time 69.03603482246399\nEpoch 10 , Train Loss 0.83102, Used Time 68.72866415977478\nEpoch 11 , Train Loss 0.82750, Used Time 68.41314888000488\nEpoch 12 , Train Loss 0.82441, Used Time 69.18851280212402\nEpoch 13 , Train Loss 0.82176, Used Time 69.12681889533997\nEpoch 14 , Train Loss 0.81948, Used Time 69.20515251159668\n","name":"stdout"}],"source":"net = SkipGram(vocab_size, 100)\nloss_fn = nn.BCEWithLogitsLoss(reduction='none')\noptimizer = optim.Adam(net.parameters(), lr=0.01)\ntrain_skip_gram(15, net, optimizer, loss_fn)","execution_count":30},{"metadata":{"id":"BC6D6AD2EB674E72856BD26C334C7B09","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":16},{"metadata":{"id":"A78F88AC90A64D938C8E92F5102BAA94","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def get_similar_words(token, embed, k):\n    token_embedding = embed[token2idx[token]].view(-1, 1)\n    weight = torch.matmul(embed, token_embedding).squeeze(1)/torch.sqrt(torch.sum(torch.mul(embed, embed), dim=1)*torch.sum(torch.mul(token_embedding, token_embedding))+1e-9)\n    values, topks = torch.topk(weight, k+1)\n    for i in range(k+1):\n        print('cosine similarity = %s, word is %s'%(values[i], idx2token[topks[i]]))\n        ","execution_count":36},{"metadata":{"id":"88FF1B64BFB34AF88C27694519F31E76","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"cosine similarity = tensor(1.), word is chip\ncosine similarity = tensor(0.5681), word is intel\ncosine similarity = tensor(0.5476), word is chips\ncosine similarity = tensor(0.4699), word is microprocessor\ncosine similarity = tensor(0.4669), word is bugs\ncosine similarity = tensor(0.4593), word is tasks\n","name":"stdout"}],"source":"embed = net.input_embedding.weight.data\nget_similar_words('chip', embed, 5)","execution_count":37},{"metadata":{"id":"181E6C5DE2CE4FDC8C838CA1F8F6AF88","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}